---
title: "TRGN_510_final"
author: "bo"
date: "11/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```
### load libraries we need
```{r}
library(MLSeq)
library(DESeq2)
library(dplyr)
```
### Create a new function to put each htseqcounts data as a df
### This function can help us input a htseq dataset and make it a right dataframe. 
```{r}
predata <- function(val) {
  df = myfilelist[val]
  df <- as.data.frame(df)
  df <- data.frame(df[,-1], row.names = df[,1])
  colnames(df) <- myfilename[val]
  print(df)
}
```
## Prepare the input data
### First we put all stage_i datasets
### We get the filepath and filename first
```{r}
mypath <- "./training_data/stage_i_htseqdata/"
myfilepath <- list.files("./training_data/stage_i_htseqdata/", pattern = "htseq", full.names = TRUE)
myfilename <- list.files(mypath, pattern = "htseq")
```
### Use lapply to make a list of our data
```{r}
myfilelist <- lapply(myfilepath, read.table)
```
### Use a loop to merge all the datasets from the filepath
```{r}
p <- predata(1)
for (i in 2:20){
  p=merge(p,predata(i),by = 0, all = T)
  p <- data.frame(p[,-1], row.names = p[,1])
}
```
### Second we put all stage_iia datasets, which is exactly same way as stage_i datasets
```{r}
mypath <- "training_data/stage_iia_htseqdata/"
myfilepath <- list.files(mypath,pattern = "htseq", full.names = TRUE)
myfilename <- list.files(mypath, pattern = "htseq")
myfilelist <- lapply(myfilepath, read.table)
q <- predata(1)
library(dplyr)
for (i in 2:20){
  q=merge(q,predata(i),by = 0, all = T)
  q <- data.frame(q[,-1], row.names = q[,1])
}
```
### Then we merge the two stage df together, and get rid of the summary rows
```{r}
df_all=merge(p,q,by=0,all=T)
df_all <- data.frame(df_all[,-1], row.names = df_all[,1])
df_all_1 <- df_all[6:nrow(df_all),]
df <- df_all_1
```
## Find the top 100 significant genes
### we create class of the our df_all(the first 20 datasets are stage i data)
```{r}
class <- data.frame(condition = factor(rep(c("i","iia"),c(20,20))))
class
```
### DESeq2, using DESeq2 to create input data of training and testing
### We use set.seed to get the same results
### we use sort to find the top 100 gene names
### we use ceiling to round up to the integer
### Test datasets : Trainning datasets= 3 : 7
### We use sample to get a specified size of datasets, here is the test dataset size.
```{r}
set.seed(2128)
vars <- sort(apply(df, 1, var, na.rm = TRUE), decreasing = TRUE)
data <- df[names(vars)[1:100],]
nTest <- ceiling(ncol(data) * 0.3)
ind <- sample(ncol(data),nTest,FALSE)
```
### Minimum count is set to 1 in order to prevent 0 division problem withinclassification models
### Add condition 
```{r}
data.train <- as.matrix(data[ ,-ind] + 1)
data.test <- as.matrix(data[ ,ind] + 1)
classtr <- DataFrame(condition = class[-ind, ])
classts <- DataFrame(condition = class[ind, ])
```
### Now we have the traning and test sets
```{r}
data.trainS4 = DESeqDataSetFromMatrix(countData = data.train, colData = classtr,
design = formula(~condition))
data.testS4 = DESeqDataSetFromMatrix(countData = data.test, colData = classts,
design = formula(~condition))
```
### Using availableMethods to see what classification algorithm we have.
```{r}
availableMethods()
```
## Normalization and training the model in MLSeq
### classify helps us do the classification
### Normalize and transform data using deseq-vst and train dataset with method "svmRadial"
```{r}
set.seed(2128)
fit.svm <- classify(data = data.trainS4, method = "svmRadial",
preProcessing = "deseq-vst", ref = "iia", tuneLength = 10,
control = trainControl(method = "repeatedcv", number = 5,
repeats = 10, classProbs = TRUE))
show(fit.svm)
```
### "trained" function shows us the details of this model
### We can see what sigma and C used in this model
```{r}
trained(fit.svm)
```
### plot the tuning parameters on the model
```{r}
plot(fit.svm)
```
### Then we test the model with test data we've built.
```{r}
pred.svm <- predict(fit.svm, data.testS4)
pred.svm
```
### We can compare the prediction and the actual level of the datasets.
### We use relevel to get the prediction and actual level.
### Use table to make of table of prediction and reality
```{r}
pred.svm <- relevel(pred.svm, ref = "iia")
pred.svm
actual <- relevel(classts$condition, ref = "iia")
actual
tbl <- table(Predicted = pred.svm, Actual = actual)
tbl
```
### Use confusionMatrix from caret to make a detailed comparision
```{r}
confusionMatrix(tbl, positive = "iia")
```
## After showing how to use the MLSeq to make classifier, we try to define the control for the model, so we can compare the model
### Defining the control list for selected classifier, so we define all the method MLSeq provided.
```{r}
set.seed(2128)
ctrl.continuous <- trainControl(method = "repeatedcv", number = 5, repeats = 10)
ctrl.discrete <- discreteControl(method = "repeatedcv", number = 5, repeats = 10,tuneLength = 10)
ctrl.voom <- voomControl(method = "repeatedcv",number = 5, repeats = 10,
tuneLength = 10)
```
### Continuous classifiers, SVM and NSC
```{r}
fit.svm <- classify(data = data.trainS4, method = "svmRadial",
preProcessing = "deseq-vst", ref = "iia", tuneLength = 10,
control = ctrl.continuous)
fit.NSC <- classify(data = data.trainS4, method = "pam",
preProcessing = "deseq-vst", ref = "iia", tuneLength = 10,
control = ctrl.continuous)
fit.svm
fit.NSC
```
### Discrete classifiers
```{r}
fit.plda <- classify(data = data.trainS4, method = "PLDA", normalize = "deseq",
ref = "iia", control = ctrl.discrete)
fit.plda2 <- classify(data = data.trainS4, method = "PLDA2", normalize = "deseq",
ref = "iia", control = ctrl.discrete)
fit.nblda <- classify(data = data.trainS4, method = "NBLDA", normalize = "deseq",
ref = "iia", control = ctrl.discrete)
fit.plda
fit.plda2
fit.nblda
```
### voom-based classifiers
```{r}
#fit.voomDLDA <- classify(data = data.trainS4, method = "voomDLDA",
#normalize = "deseq", ref = "T", control = ctrl.voom)
fit.voomNSC <- classify(data = data.trainS4, method = "voomNSC",
normalize = "deseq", ref = "iia", control = ctrl.voom)
fit.voomNSC
trained(fit.voomNSC)
```

```{r}
pred.voomNSC <- predict(fit.voomNSC, data.testS4)
```

```{r}
pred.voomNSC
```
### Because the best model we found is the voomNSC, so we compare the testing outcomes with the actual level
```{r}
pred.svm <- relevel(pred.voomNSC, ref = "iia")
pred.voomNSC
actual <- relevel(classts$condition, ref = "iia")
actual
tbl <- table(Predicted = pred.voomNSC, Actual = actual)
tbl
```


### Determining possible biomarkers using sparse classifiers
```{r}
select.fit.voomNSC <- selectedGenes(fit.voomNSC)
select.fit.NSC <- selectedGenes(fit.NSC)
select.fit.plda <- selectedGenes(fit.plda)
select.fit.plda2 <- selectedGenes(fit.plda2)
select.fit.NSC
```
### Converting the possible biomarker from selectedGenes into df
### inner join these biomarker, because NSC model does not have possible biomarker, so we join other three models
```{r}
class(select.fit.voomNSC)
voomNSC <- data.frame(value = select.fit.voomNSC)
NSC <- data.frame(value = select.fit.NSC)
plda <- data.frame(value = select.fit.plda)
plda2 <- data.frame(value = select.fit.plda2)
library(plyr)
at_the_end <- join(plda,plda2, type = "inner")
at_the_end <- join(at_the_end, voomNSC, type= "inner")
```
### At the end, we add more information to possible biomarkers using BiomaRt
```{r}
library(biomaRt)
library(dplyr)
ensembl <- useMart("ensembl")
datasets <- listDatasets(ensembl)
mart <- useDataset("hsapiens_gene_ensembl", mart = ensembl)
res_df.biomarker <- at_the_end 
res_df.biomarker
listAttributes(mart)
Biomarker <- getBM(values = res_df.biomarker$value, filters = "ensembl_gene_id_version", attributes = c("ensembl_gene_id_version", "entrezgene_id", "description", 'hgnc_symbol'), mart = mart)
Biomarker
```


